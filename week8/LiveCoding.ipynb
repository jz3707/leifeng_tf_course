{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import data\n",
    "import config\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.ops import math_ops\n",
    "from tensorflow.python.ops import control_flow_ops\n",
    "from tensorflow.python.ops import tensor_array_ops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bucketing conversation number 9999\n",
      "Bucketing conversation number 19999\n",
      "Bucketing conversation number 29999\n",
      "Bucketing conversation number 39999\n",
      "Bucketing conversation number 49999\n",
      "Bucketing conversation number 59999\n",
      "Bucketing conversation number 69999\n",
      "Bucketing conversation number 79999\n",
      "Bucketing conversation number 89999\n",
      "Bucketing conversation number 99999\n",
      "Bucketing conversation number 109999\n",
      "Bucketing conversation number 119999\n",
      "Bucketing conversation number 129999\n",
      "Bucketing conversation number 139999\n",
      "Bucketing conversation number 149999\n",
      "Bucketing conversation number 159999\n",
      "Bucketing conversation number 169999\n",
      "Bucketing conversation number 179999\n",
      "Bucketing conversation number 189999\n"
     ]
    }
   ],
   "source": [
    "buckets = data.load_data(\"train_ids.enc\", \"train_ids.dec\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BUCKETS:  [(8, 10), (12, 14), (16, 19)]\n",
      "ENC_VOCAB:  24474\n",
      "DEC_VOCAB:  24683\n",
      "HIDDEN_UNITS: 256\n",
      "LR: 0.001\n"
     ]
    }
   ],
   "source": [
    "print(\"BUCKETS: \", config.BUCKETS)\n",
    "print(\"ENC_VOCAB: \", config.ENC_VOCAB)\n",
    "print(\"DEC_VOCAB: \", config.DEC_VOCAB)\n",
    "print(\"HIDDEN_UNITS:\", config.HIDDEN_SIZE)\n",
    "print(\"LR:\", config.LR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[2, 3751, 3],\n",
       " [2, 8, 172, 97, 31, 3476, 9, 3],\n",
       " [2, 21, 1130, 9, 3],\n",
       " [2, 10, 71, 231, 8, 71, 4, 3],\n",
       " [2, 49, 36, 864, 19, 986, 18803, 9, 3],\n",
       " [2, 99, 5, 15, 73, 4, 3],\n",
       " [2, 45, 32, 37, 20, 3],\n",
       " [2, 34, 4, 3],\n",
       " [2, 8, 216, 376, 65, 52, 14, 3],\n",
       " [2, 1715, 4, 3]]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(map(lambda i: i[1], buckets[0][:10]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import abc\n",
    "\n",
    "\n",
    "class ChatBotModelBase(metaclass=abc.ABCMeta):\n",
    "    @abc.abstractmethod\n",
    "    def encode(self):\n",
    "        raise NotImplemented\n",
    "    \n",
    "    @abc.abstractmethod\n",
    "    def decode(self, enc_outputs, enc_final_state):\n",
    "        raise NotImplemented\n",
    "    \n",
    "    @abc.abstractmethod\n",
    "    def create_loss(self):\n",
    "        raise NotImplemented"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BasicChatBotModel(ChatBotModelBase):\n",
    "    def __init__(self, batch_size=64):\n",
    "        self.source_seq_tensor = tf.placeholder(tf.int32, shape=[None, batch_size], name=\"source_seq_tensor\") # [Time, Batch]\n",
    "        self.target_seq_tensor = tf.placeholder(tf.int32, shape=[None, batch_size], name=\"target_seq_tensor\") # [Time, Batch]\n",
    "        self.target_length = tf.placeholder(tf.int32, shape=(), name=\"target_length\")\n",
    "        self.decoder_seq_length = tf.placeholder(tf.int32, shape=(batch_size,), name=\"decoder_seq_length\")\n",
    "        self.global_step = tf.contrib.framework.get_global_step()\n",
    "        self.batch_size = batch_size\n",
    "    \n",
    "    def build(self):\n",
    "        enc_outputs, enc_final_state = self.encode()\n",
    "        self.final_outputs, final_state = self.decode(enc_outputs, enc_final_state)\n",
    "        self.train_op = self.create_loss()\n",
    "    \n",
    "    def encode(self):\n",
    "        with tf.variable_scope('encoder') as scope:\n",
    "            scope.set_initializer(tf.random_uniform_initializer(-0.1, 0.1))\n",
    "            \n",
    "            W = tf.get_variable(name=\"W\", shape=[config.ENC_VOCAB, config.HIDDEN_SIZE], dtype=tf.float32)\n",
    "            source_embedded = tf.nn.embedding_lookup(W, self.source_seq_tensor)\n",
    "            \n",
    "            # DropoutWrapper, LSTM, ...\n",
    "            # v1.2\n",
    "            # before v1.2, use tf.contrib.rnn.rnn_cell\n",
    "            cell = tf.nn.rnn_cell.GRUCell(num_units=config.HIDDEN_SIZE)\n",
    "            \n",
    "            enc_outputs, enc_final_state = tf.nn.dynamic_rnn(cell=cell, inputs=source_embedded, time_major=True, dtype=tf.float32)\n",
    "            \n",
    "            return enc_outputs, enc_final_state\n",
    "\n",
    "    def decode(self, enc_outputs, enc_final_state):\n",
    "        with tf.variable_scope('decoder') as scope:\n",
    "            scope = tf.get_variable_scope()\n",
    "            scope.set_initializer(tf.random_uniform_initializer(-0.1, 0.1))\n",
    "\n",
    "            W = tf.get_variable(\n",
    "                name=\"W\",\n",
    "                shape=[config.DEC_VOCAB, config.HIDDEN_SIZE],\n",
    "                initializer=tf.random_uniform_initializer(-0.1, 0.1))\n",
    "            target_embedded = tf.nn.embedding_lookup(W, self.target_seq_tensor)\n",
    "\n",
    "            cell = tf.nn.rnn_cell.GRUCell(num_units=config.HIDDEN_SIZE)\n",
    "            print(\"target_embedded.get_shape(): \", target_embedded.get_shape())\n",
    "            print(\"enc_final_state.get_shape(), \", enc_final_state.get_shape())\n",
    "\n",
    "            def condition(time, all_outputs, inputs, states):\n",
    "                return time < self.target_length - 1\n",
    "                # return tf.reduce_all(self.decoder_length_tensor > time)\n",
    "\n",
    "            def body(time, all_outputs, inputs, states):\n",
    "                dec_outputs, dec_state = cell(inputs=inputs, state=states)\n",
    "                output_logits = tf.contrib.layers.fully_connected(inputs=dec_outputs, num_outputs=config.DEC_VOCAB,\n",
    "                                                                  activation_fn=None)\n",
    "                all_outputs = all_outputs.write(time, output_logits)\n",
    "\n",
    "                output_label = tf.arg_max(output_logits, dimension=1)\n",
    "                next_input = tf.nn.embedding_lookup(W, output_label)\n",
    "                next_input.set_shape((self.batch_size, config.HIDDEN_SIZE))\n",
    "\n",
    "                return time + 1, all_outputs, next_input, dec_state\n",
    "\n",
    "            output_ta = tensor_array_ops.TensorArray(dtype=tf.float32,\n",
    "                                                     size=0,\n",
    "                                                     dynamic_size=True,\n",
    "                                                     element_shape=(self.batch_size, config.DEC_VOCAB))\n",
    "\n",
    "            res = control_flow_ops.while_loop(\n",
    "                condition,\n",
    "                body,\n",
    "                loop_vars=[0, output_ta, target_embedded[0], enc_final_state],\n",
    "            )\n",
    "            final_outputs = res[1].stack()\n",
    "            final_state = res[3]\n",
    "        return final_outputs, final_state\n",
    "    \n",
    "    def create_loss(self):\n",
    "        with tf.variable_scope('loss') as scope:\n",
    "            print(\"self.final_outputs, \", self.final_outputs.get_shape())\n",
    "            losses = tf.nn.sparse_softmax_cross_entropy_with_logits(\n",
    "                logits=self.final_outputs, labels=self.target_seq_tensor[1:])\n",
    "            mask = tf.sequence_mask(self.decoder_seq_length, self.target_length - 1)\n",
    "            print(losses.get_shape())\n",
    "            losses = losses * tf.transpose(tf.to_float(mask), (1, 0))\n",
    "            self.loss = tf.reduce_sum(losses) / tf.to_float(tf.reduce_sum(self.decoder_seq_length -1))\n",
    "            print(self.loss.get_shape())\n",
    "            \n",
    "            self.optimizer = tf.train.AdamOptimizer(learning_rate=config.LR)\n",
    "            trainables = tf.trainable_variables()\n",
    "            self.grads = self.optimizer.compute_gradients(self.loss, trainables)\n",
    "            train_op = self.optimizer.apply_gradients(self.grads, global_step=self.global_step)\n",
    "        return train_op"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class AttentionChatBotModel(BasicChatBotModel):\n",
    "    def decode(self, enc_outputs, enc_final_state):\n",
    "        with tf.variable_scope('decoder') as scope:\n",
    "            scope = tf.get_variable_scope()\n",
    "            scope.set_initializer(tf.random_uniform_initializer(-0.1, 0.1))\n",
    "\n",
    "            W = tf.get_variable(\n",
    "                name=\"W\",\n",
    "                shape=[config.DEC_VOCAB, config.HIDDEN_SIZE],\n",
    "                initializer=tf.random_uniform_initializer(-0.1, 0.1))\n",
    "            target_embedded = tf.nn.embedding_lookup(W, self.target_seq_tensor)\n",
    "\n",
    "            cell = tf.nn.rnn_cell.GRUCell(num_units=config.HIDDEN_SIZE)\n",
    "            print(\"target_embedded.get_shape(): \", target_embedded.get_shape())\n",
    "            print(\"enc_final_state.get_shape(), \", enc_final_state.get_shape())\n",
    "\n",
    "            def condition(time, all_outputs, inputs, states):\n",
    "                return time < self.target_length - 1\n",
    "                # return tf.reduce_all(self.decoder_length_tensor > time)\n",
    "\n",
    "            def body(time, all_outputs, inputs, states):\n",
    "                cell_state_input = tf.contrib.layers.fully_connected(inputs=states, num_outputs=config.HIDDEN_SIZE, activation_fn=None)\n",
    "                dec_outputs, dec_state = cell(inputs=inputs, state=cell_state_input)\n",
    "                \n",
    "                ## attention score\n",
    "                att_key = tf.contrib.layers.fully_connected(inputs=enc_outputs, num_outputs=config.CONTEXT_SIZE, activation_fn=None)\n",
    "                att_query = tf.contrib.layers.fully_connected(inputs=dec_outputs, num_outputs=config.CONTEXT_SIZE, activation_fn=None)\n",
    "                scores = tf.reduce_sum(att_key * tf.expand_dims(att_query, 0), [2])\n",
    "                scores_normalized = tf.nn.softmax(scores, dim=0)\n",
    "                \n",
    "                ## context\n",
    "                context = tf.reduce_sum(enc_outputs * tf.expand_dims(scores_normalized, 2), [0], name=\"context\")\n",
    "                \n",
    "                projection_input = tf.concat([dec_outputs, context], 1)\n",
    "                \n",
    "                output_logits = tf.contrib.layers.fully_connected(inputs=projection_input, num_outputs=config.DEC_VOCAB,\n",
    "                                                                  activation_fn=None)\n",
    "                all_outputs = all_outputs.write(time, output_logits)\n",
    "\n",
    "                output_label = tf.arg_max(output_logits, dimension=1)\n",
    "                next_input = tf.nn.embedding_lookup(W, output_label)\n",
    "                next_input.set_shape((self.batch_size, config.HIDDEN_SIZE))\n",
    "\n",
    "                return time + 1, all_outputs, next_input, tf.concat([dec_state, context], 1)\n",
    "\n",
    "            output_ta = tensor_array_ops.TensorArray(dtype=tf.float32,\n",
    "                                                     size=0,\n",
    "                                                     dynamic_size=True,\n",
    "                                                     element_shape=(self.batch_size, config.DEC_VOCAB))\n",
    "\n",
    "            res = control_flow_ops.while_loop(\n",
    "                condition,\n",
    "                body,\n",
    "                loop_vars=[0,\n",
    "                           output_ta, target_embedded[0],\n",
    "                           tf.concat(\n",
    "                               [enc_final_state,\n",
    "                               tf.zeros(dtype=tf.float32, shape=(self.batch_size, config.CONTEXT_SIZE))], 1)],\n",
    "            )\n",
    "            final_outputs = res[1].stack()\n",
    "            final_state = res[3]\n",
    "        return final_outputs, final_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bucketing conversation number 9999\n",
      "Bucketing conversation number 19999\n",
      "Bucketing conversation number 9999\n",
      "Bucketing conversation number 19999\n",
      "Bucketing conversation number 29999\n",
      "Bucketing conversation number 39999\n",
      "Bucketing conversation number 49999\n",
      "Bucketing conversation number 59999\n",
      "Bucketing conversation number 69999\n",
      "Bucketing conversation number 79999\n",
      "Bucketing conversation number 89999\n",
      "Bucketing conversation number 99999\n",
      "Bucketing conversation number 109999\n",
      "Bucketing conversation number 119999\n",
      "Bucketing conversation number 129999\n",
      "Bucketing conversation number 139999\n",
      "Bucketing conversation number 149999\n",
      "Bucketing conversation number 159999\n",
      "Bucketing conversation number 169999\n",
      "Bucketing conversation number 179999\n",
      "Bucketing conversation number 189999\n",
      "Number of samples in each bucket:\n",
      " [37961, 34335, 31129]\n",
      "Bucket scale:\n",
      " [0.3670389170896785, 0.6990186125211506, 1.0]\n"
     ]
    }
   ],
   "source": [
    "test_buckets, data_buckets, train_buckets_scale = get_buckets()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "target_embedded.get_shape():  (?, 64, 256)\n",
      "enc_final_state.get_shape(),  (?, 256)\n",
      "self.final_outputs,  (?, 64, 24683)\n",
      "(?, 64)\n",
      "()\n",
      "ERROR:tensorflow:==================================\n",
      "Object was never used (type <class 'tensorflow.python.ops.tensor_array_ops.TensorArray'>):\n",
      "<tensorflow.python.ops.tensor_array_ops.TensorArray object at 0x130306be0>\n",
      "If you want to mark it as used call its \"mark_used()\" method.\n",
      "It was originally created here:\n",
      "['File \"/Users/dtong/miniconda3/envs/leifeng/lib/python3.6/runpy.py\", line 193, in _run_module_as_main\\n    \"__main__\", mod_spec)', 'File \"/Users/dtong/miniconda3/envs/leifeng/lib/python3.6/runpy.py\", line 85, in _run_code\\n    exec(code, run_globals)', 'File \"/Users/dtong/miniconda3/envs/leifeng/lib/python3.6/site-packages/ipykernel_launcher.py\", line 16, in <module>\\n    app.launch_new_instance()', 'File \"/Users/dtong/miniconda3/envs/leifeng/lib/python3.6/site-packages/traitlets/config/application.py\", line 658, in launch_instance\\n    app.start()', 'File \"/Users/dtong/miniconda3/envs/leifeng/lib/python3.6/site-packages/ipykernel/kernelapp.py\", line 477, in start\\n    ioloop.IOLoop.instance().start()', 'File \"/Users/dtong/miniconda3/envs/leifeng/lib/python3.6/site-packages/zmq/eventloop/ioloop.py\", line 177, in start\\n    super(ZMQIOLoop, self).start()', 'File \"/Users/dtong/miniconda3/envs/leifeng/lib/python3.6/site-packages/tornado/ioloop.py\", line 888, in start\\n    handler_func(fd_obj, events)', 'File \"/Users/dtong/miniconda3/envs/leifeng/lib/python3.6/site-packages/tornado/stack_context.py\", line 277, in null_wrapper\\n    return fn(*args, **kwargs)', 'File \"/Users/dtong/miniconda3/envs/leifeng/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py\", line 440, in _handle_events\\n    self._handle_recv()', 'File \"/Users/dtong/miniconda3/envs/leifeng/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py\", line 472, in _handle_recv\\n    self._run_callback(callback, msg)', 'File \"/Users/dtong/miniconda3/envs/leifeng/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py\", line 414, in _run_callback\\n    callback(*args, **kwargs)', 'File \"/Users/dtong/miniconda3/envs/leifeng/lib/python3.6/site-packages/tornado/stack_context.py\", line 277, in null_wrapper\\n    return fn(*args, **kwargs)', 'File \"/Users/dtong/miniconda3/envs/leifeng/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 283, in dispatcher\\n    return self.dispatch_shell(stream, msg)', 'File \"/Users/dtong/miniconda3/envs/leifeng/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 235, in dispatch_shell\\n    handler(stream, idents, msg)', 'File \"/Users/dtong/miniconda3/envs/leifeng/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 399, in execute_request\\n    user_expressions, allow_stdin)', 'File \"/Users/dtong/miniconda3/envs/leifeng/lib/python3.6/site-packages/ipykernel/ipkernel.py\", line 196, in do_execute\\n    res = shell.run_cell(code, store_history=store_history, silent=silent)', 'File \"/Users/dtong/miniconda3/envs/leifeng/lib/python3.6/site-packages/ipykernel/zmqshell.py\", line 533, in run_cell\\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)', 'File \"/Users/dtong/miniconda3/envs/leifeng/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2698, in run_cell\\n    interactivity=interactivity, compiler=compiler, result=result)', 'File \"/Users/dtong/miniconda3/envs/leifeng/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2802, in run_ast_nodes\\n    if self.run_code(code, result):', 'File \"/Users/dtong/miniconda3/envs/leifeng/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2862, in run_code\\n    exec(code_obj, self.user_global_ns, self.user_ns)', 'File \"<ipython-input-56-3fbe164fb70a>\", line 8, in <module>\\n    model.build()', 'File \"<ipython-input-39-b11ab3efb017>\", line 12, in build\\n    self.final_outputs, final_state = self.decode(enc_outputs, enc_final_state)', 'File \"<ipython-input-55-df286d519120>\", line 57, in decode\\n    tf.zeros(dtype=tf.float32, shape=(self.batch_size, config.CONTEXT_SIZE))], 1)],', 'File \"/Users/dtong/miniconda3/envs/leifeng/lib/python3.6/site-packages/tensorflow/python/ops/control_flow_ops.py\", line 2770, in while_loop\\n    result = context.BuildLoop(cond, body, loop_vars, shape_invariants)', 'File \"/Users/dtong/miniconda3/envs/leifeng/lib/python3.6/site-packages/tensorflow/python/ops/control_flow_ops.py\", line 2599, in BuildLoop\\n    pred, body, original_loop_vars, loop_vars, shape_invariants)', 'File \"/Users/dtong/miniconda3/envs/leifeng/lib/python3.6/site-packages/tensorflow/python/ops/control_flow_ops.py\", line 2549, in _BuildLoop\\n    body_result = body(*packed_vars_for_body)', 'File \"<ipython-input-55-df286d519120>\", line 37, in body\\n    all_outputs = all_outputs.write(time, output_logits)', 'File \"/Users/dtong/miniconda3/envs/leifeng/lib/python3.6/site-packages/tensorflow/python/util/tf_should_use.py\", line 170, in wrapped\\n    return _add_should_use_warning(fn(*args, **kwargs))', 'File \"/Users/dtong/miniconda3/envs/leifeng/lib/python3.6/site-packages/tensorflow/python/util/tf_should_use.py\", line 139, in _add_should_use_warning\\n    wrapped = TFShouldUseWarningWrapper(x)', 'File \"/Users/dtong/miniconda3/envs/leifeng/lib/python3.6/site-packages/tensorflow/python/util/tf_should_use.py\", line 96, in __init__\\n    stack = [s.strip() for s in traceback.format_stack()]']\n",
      "==================================\n",
      "Step 0: loss - 11.440528869628906\n",
      "Step 1: loss - 11.018695831298828\n",
      "Step 2: loss - 11.42657470703125\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "from util import get_buckets\n",
    "\n",
    "tf.reset_default_graph()\n",
    "\n",
    "model = AttentionChatBotModel(batch_size=config.BATCH_SIZE)\n",
    "model.build()\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    for step in range(3):\n",
    "        rand = random.random()\n",
    "        bucket_id = min([i for i in range(len(train_buckets_scale))\n",
    "                    if train_buckets_scale[i] > rand])\n",
    "        encoder_inputs, decoder_inputs, decoder_masks = data.get_batch(\n",
    "                data_buckets[bucket_id], bucket_id, batch_size=config.BATCH_SIZE)\n",
    "        decoder_lens = np.sum(np.transpose(np.array(decoder_masks), (1, 0)), axis=1)\n",
    "        loss_res, _ = sess.run([model.loss, model.train_op], feed_dict={\n",
    "            model.source_seq_tensor: encoder_inputs,\n",
    "            model.target_seq_tensor: decoder_inputs,\n",
    "            model.target_length: config.BUCKETS[bucket_id][1],\n",
    "            model.decoder_seq_length: decoder_lens\n",
    "        })\n",
    "        print(\"Step {}: loss - {}\".format(step, loss_res))\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10.113870026547724"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
